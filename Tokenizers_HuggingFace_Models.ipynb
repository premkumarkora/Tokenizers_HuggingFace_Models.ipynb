{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPmlI18B5EqEJm63IxZIN7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/premkumarkora/Tokenizers_in_HuggingFace_Models/blob/main/Tokenizers_HuggingFace_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AutoTokenizer** is a generic class that automatically loads the correct tokenizer for any pretrained Hugging Face transformer model.\n",
        "\n",
        "It handles splitting text into subword tokens, adding special tokens (e.g., [CLS], [SEP]), and creating attention masks.\n",
        "\n",
        "Depending on the model, it uses different algorithms like WordPiece, Byte-Pair Encoding (BPE), or SentencePiece under the hood.\n",
        "\n",
        "You initialize it with tokenizer = AutoTokenizer.from_pretrained(\"model-name\"), which downloads and caches the tokenizer configuration and vocab files.\n",
        "\n",
        "After loading, use tokenizer(text, return_tensors=\"pt\") to convert raw text into model-ready input IDs (and reverse with tokenizer.decode())."
      ],
      "metadata": {
        "id": "J9rh4EcvmOLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every pretrained model on Hugging Face comes paired with a tokenizer that mirrors how it was trained:\n",
        "\n",
        "**Model-specific vocab and rules**\n",
        "Each model repository on the Hub includes tokenizer files (vocabulary, merges/rules, special-token mappings) exactly matching what the model saw during pretraining.\n",
        "\n",
        "**Algorithm varies by architecture**\n",
        "BERT-style models typically use WordPiece, GPT-style models use Byte-Pair Encoding, and others may use SentencePiece or Unigram; the AutoTokenizer you load knows which under-the-hood algorithm to pull in.\n",
        "\n",
        "**Shared across variants**\n",
        "Different checkpoints of the same architecture (e.g. bert-base-uncased vs. bert-large-uncased) share the same tokenizer type and vocab, but fine-tuned or multilingual variants may have expanded or modified vocabularies.\n",
        "\n",
        "**Consistency is crucial**\n",
        "Using the exact tokenizer used at pretraining ensures that token‐to‐ID mappings match the model’s learned embeddings—mismatched tokenizers will produce incorrect inputs and degrade performance.\n",
        "\n",
        "AutoTokenizer management\n",
        "\n",
        "`tokenizer = AutoTokenizer.from_pretrained(\"model-name\")`\n",
        "\n",
        "it fetches the right tokenizer files for that model so you don’t have to worry about manually specifying vocab paths or algorithms."
      ],
      "metadata": {
        "id": "Ta6W4cL-nbkO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m714QYPl0tH"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieves your Hugging Face API token from stored user data and authenticates the CLI session. It also adds the token to your Git credentials for seamless access to private repos."
      ],
      "metadata": {
        "id": "nS9xbR33oGSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "IHr6-kiNoFqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing Llama 3.1 from Meta"
      ],
      "metadata": {
        "id": "HdNfK43eoPD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "yPxPlUB8oOD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"You want to see how the Tokenizers work in Llama3.1 and the family of 3.1\"\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "xUSddTQYoaAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the Length of Toens generated by Llama3.1"
      ],
      "metadata": {
        "id": "HewRr9HcopWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "i_eHa_jooqSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "id": "kKV0NK4Hpvvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text.split())"
      ],
      "metadata": {
        "id": "TnMZGp-2pZvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding the tokens"
      ],
      "metadata": {
        "id": "iUhQPSzmp705"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "rZc7PVsip7gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thought for 5 seconds\n",
        "\n",
        "\n",
        "The call to `tokenizer.batch_decode(tokens)` takes your sequence of token IDs (or token strings) and maps them back into human‐readable text. It:\n",
        "\n",
        "1. Converts each token ID into its corresponding string piece (including special tokens like `<|begin_of_text|>`).\n",
        "2. Joins subword fragments (e.g. `\"Token\"` + `\"izers\" → \"Tokenizers\"`) into full words.\n",
        "3. Reconstructs the original text sequence you tokenized.\n"
      ],
      "metadata": {
        "id": "8cNv02maqVeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "NjPRTTi6qEKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thought for a few seconds\n",
        "\n",
        "\n",
        "The method `tokenizer.get_added_vocab()` returns a dictionary of all tokens you’ve dynamically added to the tokenizer (via `tokenizer.add_tokens(...)`), mapping each new token string to its assigned token ID. If you haven’t added any extra tokens, it will return an empty dict.\n"
      ],
      "metadata": {
        "id": "cFh3vEM6qcrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "qom140vMqcdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tBt4yduBrHdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "7fIgwHHRrH23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many models have a variant that has been trained for use in Chats.\n",
        "These are typically labelled with the word \"Instruct\" at the end.\n",
        "They have been trained to expect prompts with a particular format that includes system, user and assistant prompts."
      ],
      "metadata": {
        "id": "7Zpx2djOsYSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Define the chat history**\n",
        "   You create `messages`, a list of dictionaries where each entry has a `\"role\"` (like `system` or `user`) and its `\"content\"`.\n",
        "\n",
        "2. **Build a single prompt string**\n",
        "   `apply_chat_template(...)` takes that list and stitches it into one long string, inserting special tokens around each role (e.g. `<|start_header_id|>system<|end_header_id|>`) plus metadata lines (knowledge cut-off date, today’s date).\n",
        "\n",
        "3. **Add the “assistant” cue**\n",
        "   By setting `add_generation_prompt=True`, it tacks on the final `<|start_header_id|>assistant…` marker so the model knows “now it’s your turn to talk.”\n",
        "\n",
        "4. **Result**\n",
        "   When you `print(prompt)`, you see one continuous text blob that the model can consume directly—complete with role markers, context dates, and a generation slot for the assistant’s reply.\n"
      ],
      "metadata": {
        "id": "-Yzb7g__rt4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "XJojLYyorKwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phi3 from Microsoft\n",
        "\n",
        "Qwen2 from Alibaba Cloud\n",
        "\n",
        "Starcoder2 from BigCode (ServiceNow + HuggingFace + NVidia)"
      ],
      "metadata": {
        "id": "UR7Gc-iism-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
        "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
      ],
      "metadata": {
        "id": "iOvsJSZwsr6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action for PHI3 Model\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "id": "FiNDH38asy3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "0krIJNAfs7aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qwen"
      ],
      "metadata": {
        "id": "5XrBd6iUtQkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "print(phi3_tokenizer.encode(text))\n",
        "print()\n",
        "print(qwen2_tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "mjOQCP6CtS2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "OAFyUZsEtehd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n",
        "code = \"\"\"\n",
        "def hello_world(person):\n",
        "  print(\"Hello\", person)\n",
        "\"\"\"\n",
        "tokens = starcoder2_tokenizer.encode(code)\n",
        "for token in tokens:\n",
        "  print(f\"{token}={starcoder2_tokenizer.decode(token)}\")"
      ],
      "metadata": {
        "id": "WYGrrMB0tmTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}